# k8s ingress metrics
# [[inputs.prometheus]]
#     interval = "1m"
#     urls = $K8S_INGRESS_URLS

# middleware
[[inputs.elasticsearch]]
    servers = $ELASTICSEARCH_SERVERS
    http_timeout = "8s"
    local = $ELASTICSEARCH_GATHER_LOCAL
    cluster_health = true
    cluster_health_level = "cluster"
    cluster_stats = false
    node_stats = ["indices", "transport", "http", "jvm", "fs", "os", "process"]
    [inputs.elasticsearch.tags]
      addon_id = "elasticsearch"
      addon_type = "elasticsearch"

# [[inputs.redis]]
#     servers = $REDIS_SERVERS

[[inputs.zookeeper]]
    servers = $ZOOKEEPER_SERVERS
    [inputs.zookeeper.tags]
      addon_id = "zookeeper"
      addon_type = "zookeeper"

# mysql
[[inputs.mysql]]
    servers = $MYSQL_SERVERS
    metric_version = 2
    perf_events_statements_digest_text_limit  = 120
    perf_events_statements_limit              = 250
    perf_events_statements_time_limit         = 86400
    table_schema_databases                    = []
    gather_table_schema                       = false
    gather_process_list                       = false
    gather_user_statistics                    = false
    gather_info_schema_auto_inc               = false
    gather_innodb_metrics                     = false
    gather_slave_status                       = false
    gather_binary_logs                        = false
    gather_table_io_waits                     = false
    gather_table_lock_waits                   = false
    gather_index_io_waits                     = false
    gather_event_waits                        = false
    gather_file_events_stats                  = false
    gather_perf_events_statements             = false
   # interval_slow                            = "30m"
    fieldpass = [ 
		"threads_connected",
		"queries", "questions", "slow_queries", 
		"com_commit",  "com_rollback", 
		"com_select", "com_insert", "com_update", "com_delete", 
		"innodb_data_read", "innodb_data_reads", "innodb_data_written", "innodb_data_writes",
		"bytes_sent", "bytes_received",
		"innodb_rows_deleted", "innodb_rows_inserted", "innodb_rows_read", "innodb_rows_updated", "uptime", 
        "open_tables", "threads_running","table_locks_waited"]
    [inputs.mysql.tags]
      addon_id = "mysql"
      addon_type = "mysql"

[[inputs.mysql_size_summary]]
    servers = $MYSQL_SERVERS
    per_database  = false
    [inputs.mysql_size_summary.tags]
      addon_id = "mysql"
      addon_type = "mysql"

[[inputs.jolokia2_agent]]
  urls = $KAFKA_JOLOKIA_URLS
  name_prefix = "addon_java_"
  [inputs.jolokia2_agent.tags]
    addon_id = "kafka"
    addon_type = "kafka"

  [[inputs.jolokia2_agent.metric]]
    name  = "Memory"
    mbean = "java.lang:type=Memory"
  [[inputs.jolokia2_agent.metric]]
    name  = "GarbageCollector"
    mbean = "java.lang:name=*,type=GarbageCollector"
    tag_keys = ["name"]
    paths = ["CollectionCount", "CollectionTime", "Name", "Valid"]

[[inputs.jolokia2_agent]]
  urls = $KAFKA_JOLOKIA_URLS
  name_prefix = "kafka_"
  [inputs.jolokia2_agent.tags]
    addon_id = "kafka"
    addon_type = "kafka"

  [[inputs.jolokia2_agent.metric]]
    name         = "controller"
    mbean        = "kafka.controller:name=*,type=*"
    field_prefix = "$1."
  [[inputs.jolokia2_agent.metric]]
    name         = "replica_manager"
    mbean        = "kafka.server:name=*,type=ReplicaManager"
    field_prefix = "$1."
  [[inputs.jolokia2_agent.metric]]
    name         = "purgatory"
    mbean        = "kafka.server:delayedOperation=*,name=*,type=DelayedOperationPurgatory"
    field_prefix = "$1."
    field_name   = "$2"
  [[inputs.jolokia2_agent.metric]]
    name     = "client"
    mbean    = "kafka.server:client-id=*,type=*"
    tag_keys = ["client-id", "type"]
  [[inputs.jolokia2_agent.metric]]
    name         = "request"
    mbean        = "kafka.network:name=*,request=*,type=RequestMetrics"
    field_prefix = "$1."
    tag_keys     = ["request"]
  [[inputs.jolokia2_agent.metric]]
    name         = "topics"
    mbean        = "kafka.server:name=*,type=BrokerTopicMetrics"
    field_prefix = "$1."
  [[inputs.jolokia2_agent.metric]]
    name         = "topic"
    mbean        = "kafka.server:name=*,topic=*,type=BrokerTopicMetrics"
    field_prefix = "$1."
    tag_keys     = ["topic"]
  [[inputs.jolokia2_agent.metric]]
    name       = "partition"
    mbean      = "kafka.log:name=*,partition=*,topic=*,type=Log"
    field_name = "$1"
    tag_keys   = ["topic", "partition"]
  [[inputs.jolokia2_agent.metric]]
    name       = "partition"
    mbean      = "kafka.cluster:name=UnderReplicated,partition=*,topic=*,type=Partition"
    field_name = "UnderReplicatedPartitions"
    tag_keys   = ["topic", "partition"]

[[processors.point_space_to_underline]]
    namepass = ["addon_java_*", "cassandra_*", "kafka_*"]

[[inputs.kafka_monitor]]
    interval = "1m"
    servers = "$BOOTSTRAP_SERVERS"
    [inputs.kafka_monitor.tags]
        addon_id = "kafka"
        addon_type = "kafka"

[[inputs.prometheus]]
  urls = $ETCD_URLS
  name_override = "etcd"
  metric_version = 2
  tls_ca = "/netdata/dice-ops/dice-config/certificates/etcd-ca.pem"
  tls_cert = "/netdata/dice-ops/dice-config/certificates/etcd-client.pem"
  tls_key = "/netdata/dice-ops/dice-config/certificates/etcd-client-key.pem"
  insecure_skip_verify = true
  [inputs.prometheus.tags]
    addon_id = "etcd"
    addon_type = "etcd"
    edge_cluster = "true"

[[inputs.kube_inventory]]
  interval = "1m"
  namespace = "kube-system"
  url = "$MASTER_VIP_URL"
  # nodes, persistentvolumeclaims, persistentvolumes with all namespace
  resource_include = [ "nodes", "daemonsets", "statefulsets", "deployments", "persistentvolumes", "persistentvolumeclaims"]
  bearer_token = "/run/secrets/kubernetes.io/serviceaccount/token"
  tls_ca = "/run/secrets/kubernetes.io/serviceaccount/ca.crt"

[[inputs.kube_inventory]]
  interval = "1m"
  namespace = "default"
  url = "$MASTER_VIP_URL"
  resource_include = [ "daemonsets", "statefulsets", "deployments" ]
  bearer_token = "/run/secrets/kubernetes.io/serviceaccount/token"
  tls_ca = "/run/secrets/kubernetes.io/serviceaccount/ca.crt"

[[inputs.prometheus]]
  interval = "5m"
  kubernetes_services = ["http://$CASSANDRA_EXPORTER_JMX_ADDR:1234/metrics"]
  name_override = "cassandra"
  metric_version = 2
  url_tag = ""
  fieldpass = [
    "cassandra_columnfamily_totaldiskspaceused",
    # microseconds
    "cassandra_table_writelatency_95thpercentile",
    # write count
    "cassandra_table_writelatency_count",
    # microseconds
    "cassandra_table_readlatency_95thpercentile",
    # read count
    "cassandra_table_readlatency_count",
    "cassandra_cache_hitrate",
    # The disk space used by live data on a node. bytes
    "cassandra_storage_load",
    "cassandra_compaction_completedtasks",
    "cassandra_compaction_pendingtasks",
  ]
  [inputs.prometheus.tagpass]
    keyspace = ["spot_*"]

#  coredns
[[inputs.prometheus]]
  interval = "1m"
  urls = ["http://$CLUSTER_DNS:9153"]
  name_override = "coredns"
  metric_version = 2
  fieldpass = ["coredns_*"]
  [inputs.prometheus.tags]
    edge_cluster = "true"

# k8s apiserver
[[inputs.prometheus]]
  interval = "5m"
  urls = ["$MASTER_VIP_URL"]
  name_override = "kubernetes_apiserver"
  metric_version = 2
  fieldpass = [
    "apiserver_request_duration_seconds",
    "apiserver_request_total",
    "workqueue_depth",
    "workqueue_adds_total",
    "workqueue_queue_duration_seconds",
    "go_goroutines"
  ]

# ingress
#[[inputs.prometheus]]
#  interval = "5m"
#  name_override = "kubernetes_ingress"
#  metric_version = 2
#  fieldpass = [
#    "nginx_ingress_controller_requests",
#    "nginx_ingress_controller_ingress_upstream_latency_seconds*",
#  ]
#  bearer_token = "${K8S_BEARER_TOKEN:/run/secrets/kubernetes.io/serviceaccount/token}"
#  tls_ca = "${K8S_TLS_CA:/run/secrets/kubernetes.io/serviceaccount/ca.crt}"
#  # select pod address by label
#  [inputs.prometheus.kubernetes_selector]
#    enable = true
#    port = "10254"
#    namespace = "kube-system"
#  [inputs.prometheus.kubernetes_selector.label]
#    "app.kubernetes.io/name" = "ingress-nginx"

# kube controller
#[[inputs.prometheus]]
#  interval = "1m"
#  name_override = "kubernetes_controller"
#  metric_version = 2
#  fieldpass = [
#    "workqueue_queue_duration_seconds*",
#    "workqueue_adds_total",
#    "workqueue_depth",
#    "rest_client_request_latency_seconds",
#    "rest_client_requests_total"
#  ]
#  bearer_token = "${K8S_BEARER_TOKEN:/run/secrets/kubernetes.io/serviceaccount/token}"
#  tls_ca = "${K8S_TLS_CA:/run/secrets/kubernetes.io/serviceaccount/ca.crt}"
#  # select pod address by label
#  [inputs.prometheus.kubernetes_selector]
#    enable = true
#    port = "10252"
#    namespace = "kube-system"
#  [inputs.prometheus.kubernetes_selector.label]
#    "component" = "kube-controller-manager"

[[inputs.dice_health]]
  interval = "2m"
  exclude = [
      "fdp-ui",
      "mesh-controller",
      "tmc",
      "apim",
      "analyzer-alert",
      "analyzer-alert-task",
      "analyzer-error-insight",
      "analyzer-error-insight-task",
      "analyzer-metrics",
      "analyzer-metrics-task",
      "filebeat",
      "cloud-import",
      "fdp-agent",
      "fdp-master",
      "uc-adaptor"
    ]
  kubernetes_label_selector = "dice/component"
  [inputs.dice_health.service_check]
    timeout = "30s"

[[inputs.prometheus]]
    interval = "5m"
    urls = ["http://${MONITOR_ADMIN_ADDR:monitor:7098}"]
    name_override = "erda_monitor"
    metric_version = 2
    fielddrop = [
      "go_*",
      "promhttp_*",
      "process_*"
    ]

# [[inputs.spark]]
#   # urls = []
#   ## Set response_timeout (default 5 seconds)
#   timeout = "10s"
#   job_include = true
#   stage_include = false
#
#   [inputs.spark.k8s_service_discovery]
#     port = 4040
#     namespace = "default"
#     name_include = ["spark-thrift-server*"]
#     protocol = "TCP"

[[inputs.global_kubernetes]] # 共享 kubernetes client, 不主动采集数据

